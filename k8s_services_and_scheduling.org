#+title: Services and Scheduling

Services are objects in K8s. Services enable the communication between various component within and outside the application.


It is services enables the communication between the various groups of the pods. Services enables the loose coupling beween the
microservices.

There are 3 types of Services

 - NodePort

   If we have deployed the application on pod on one of the node and we want to access the application running on that pod from our laptop
   or from any part of the world. This is where service commes into play one of the use case is to listen on a port and forward the request
   to a port on the pod this is NodePort service. Service maps the port on the node to port on the pod. Service is like a virtual server in a cluster.

   * Target Port (Port on which application running in a pod)
   * Port
   * NodePort (30000, 32767)

   Porta are array.

   Labels and selectors to link the service to pods. Service acts as a load balancer to distribute request across the pods.

 - ClusterIP

   The service creates a virtual IP in a cluster to enable the commmunication between the pods. This makes the internal commmmunication makes simpler. It provides the interface.
   Each service gets the IP and assign a name in the cluster which can be use by other services.
   The service is only accessible from within the Kubernetes cluster – you can’t make requests to your Pods from outside the cluster!

 - Load Balancer

   The service becomes accessible externally through a cloud provider's load balancer functionality. GCP, AWS, Azure, and OpenStack offer this functionality.
   The cloud provider will create a load balancer, which then automatically routes requests to your Kubernetes Service


* Scheduling

The Kubernetes Scheduler is a core component of Kubernetes: After a user or a controller creates a Pod, the Kubernetes Scheduler, monitoring the Object Store for unassigned Pods, will assign the Pod to a Node.
Then, the Kubelet, monitoring the Object Store for assigned Pods, will execute the Pod.



The task of the Kubernetes Scheduler is to choose a placement. A placement is a partial, non-injective assignment of a set of Pods to a set of Nodes.

Scheduling is an optimization problem: First, the Scheduler determines the set of feasible placements, which is the set of placements that meet a set of given constraints.
Then, the Scheduler determines the set of viable placements, which is the set of feasible placements with the highest score.


*** Do You Have What it Takes To Run This Pod (Predicate)?

A node may be overloaded with so many busy pods consuming most of its CPU and memory. So, when the scheduler has a Pod to deploy, it determines whether or not the node has the necessary resources. If a Pod was deployed to a node that does not have enough memory (for example) that the Pod is requesting, the hosted application might behave unexpectedly or even crash.

Sometimes, the user needs to make this decision on behalf of Kubernetes. Let’s say that you’ve recently purchased a couple of machines equipped with SSD disks, and you want to use them explicitly for the MongoDB part of the application. To do this, you select the nodes through the node labels in the pod definition. When a node does not match the provided label, it is not chosen for deploying the Pod.

*** Are You a Better Candidate For Having This Pod (Priorities)?

In addition to true/false decisions a.k.a predicates, the scheduler executes some calculations (or functions) to determine which node is more suited to be hosting the pod in question.

For example, a node where the pod image is already present (like it’s been pulled before in a previous deployment) has a better chance of having the pod scheduled to it because no time will be wasted downloading the image again.

Another example is when the scheduler favors a node that does not include other pods of the same Service. This algorithm helps spread the Service pods on multiple nodes as much as possible so that one node failure does not cause the entire Service to go down. Such a decision-making method is called the spreading function.

Manual Scheduling

When you do dnot want to rely on inbuilt scheduler and wants to schedule pods on your own.
By default each pods have a field name nodename that by default not set, we dont set this filed k8s adds it automatically.
Scheduler goes through all the pods which dont have nodename not set those are the candidates for scheduling
than identify the right nodes for the pods by running the shceduling algorithm.

If there is no  scheduler  pods will always remain in pennding states.

The another way to assign  a node  request we can use binding object.


* Labels and Selectors

Labels are key-value pairs which are attached to pods, replication controller and services.
They are used as identifying attributes for objects such as pods and replication controller.
They can be added to an object at creation time and can be added or modified at the run time.
Like all other Kubernetes resources you can create your label using the YAML file or using the kubectl command.

The label specified is found in the metadata section of your YAML file

The YAML file below has a label [environment: production] and a label

* Selectors

Selectors are basically filters to filter the labels.

Labels do not provide uniqueness. In general, we can say many objects can carry the same labels. Labels selector are core grouping primitive in Kubernetes. They are used by the users to select a set of objects.

Kubernetes API currently supports two type of selectors −

- Equality-based selectors

Equality- or inequality-based requirements allow filtering by label keys and values. Matching objects must satisfy all of the specified label constraints, though they may have additional labels as well.
Three kinds of operators are admitted =,==,!=. The first two represent equality (and are simply synonyms), while the latter represents inequality. For example:

#+BEGIN_SRC
environment = production
tier != frontend
#+END_SRC


The former selects all resources with key equal to environment and value equal to production.

The latter selects all resources with key equal to tier and value distinct from frontend, and all resources with no labels with the tier key.
One could filter for resources in production excluding frontend using the comma operator: environment=production,tier!=frontend

- Set-based selectors

Set-based label requirements allow filtering keys according to a set of values. Three kinds of operators are supported: in,notin and exists (only the key identifier). For example:

#+BEGIN_SRC
environment in (production, qa)
tier notin (frontend, backend)
partition
!partition
#+END_SRC

The first example selects all resources with key equal to environment and value equal to production or qa.
The second example selects all resources with key equal to tier and values other than frontend and backend, and all resources with no labels with the tier key.
The third example selects all resources including a label with key partition; no values are checked. The fourth example selects all resources without a label with key partition; no values are checked.
Similarly the comma separator acts as an AND operator. So filtering resources with a partition key (no matter the value) and with environment different than
qa can be achieved using partition,environment notin (qa).
The set-based label selector is a general form of equality since environment=production is equivalent to environment in (production); similarly for != and notin.

* Annotations

Annotations are quite similar to labels only that annotations provide a place to store additional metadata for Kubernetes objects with the main purpose of assisting tools and libraries.
The key for annotations would be something like this

deployment.kubernetes.io/revision or kubernetes.io/change-cause.

If you use git for version control, annotations are like the git commits, where each git commit specifies which changes you made to your code.

The official Kubernetes documentation page gives us examples of information we can keep in our annotations.

    Fields managed by a declarative configuration layer. Attaching these fields as annotations distinguishes them from default values set by clients or servers, and from auto-generated fields and fields set by auto-sizing or auto-scaling systems.
    Build, release, or image information like timestamps, release IDs, git branch, PR numbers, image hashes, and registry address.
    Pointers to logging, monitoring, analytics, or audit repositories.
    Client library or tool information that can be used for debugging purposes

* Liveliness and Readiness Probe

What does it mean to self heal?

When a pod is scheduled to a node, the kubelet on that node runs its containers and keeps them running as long as the pod exists. The kubelet will restart a container if its main process crashes. But if the application inside of the container throws an error which causes it to continuously restart, Kubernetes has the ability to heal it by using the correct diagnostic and then following the pod’s restart policy.

Within containers, the kubelet can react to two kinds of probes:

Liveness and Readiness probes are Kubernetes capabilities that enable teams to make their containerised applications more reliable and robust.
However, if used inappropriately they can result in none of the intended benefits, and can actually make a microservice based application unstable.


    Liveness : Under what circumstances is it appropriate to restart the pod?
    Readiness : under what circumstances should we take the pod out of the list of service endpoints so that it no longer responds to requests?

    ExecAction - executes a command inside the container.
    TCPSocketAction - performs a TCP check against the container’s IP address on a specified port.
    HTTPGetAction - performs an HTTP GET request on the container’s IP.

    A handler can then return the following:

    Success - the diagnostic passed on the container.
    Fail - the container failed the diagnostic and will restart according to its restart policy.
    Unknown - the diagnostic failed and no action will be taken.

* K8s QOS

*** Resources Types

Kubernetes has only two built-in manageable resources: CPU and memory. CPU base units are cores, and memory is specified in bytes.
These two resources play a critical role in how the scheduler allocate pods to nodes. Memory and CPU can be requested, allocated, and consumed.
You should always set the right CPU memory values. You will be in control of your cluster and make sure that a misbehaving application does not impact capacity available for other pods in your cluster.

*** Requests & Limits

Kubernetes uses the requests & limits structure to control resources such as CPU and memory.

    Requests are what the container is guaranteed to get. For example, If a container requests a resource, Kubernetes will only schedule it on a node that can give it that resource.
    Limits, on the other hand, is the resource threshold a container never exceed. The container is only allowed to go up to the limit, and then it is restricted.

CPU is a compressible resource, which means that once your container reaches the limit, it will keep running but the operating system will throttle it and keep de-scheduling from using the CPU. Memory, on the other hand, is none compressible resource. Once your container reaches the memory limit, it will be terminated, aka OOM (Out of Memory) killed. If your container keeps OOM killed, Kubernetes will report that it is in a crash loop.

The limit can never be lower than the request. Kubernetes will throw an error and won’t let you run the container if your limit is higher than the request.

By default, all resources in Kubernetes cluster are created in a default namespace. A pod will run with unbounded CPU and memory requests/limits.

A Kubernetes namespace allows one to partition created resources into a logically named group. Each namespace provides:

    a unique scope for resources to avoid name collisions
    policies to ensure appropriate authority to trusted users
    ability to specify constraints for resource consumption

This allows a Kubernetes cluster to share resources by multiple groups and provide different levels of QoS for each group.

Resources created in one namespace are hidden from other namespaces. Multiple namespaces can be created, each potentially with different constraints.


Quality of Service (QoS) class is a Kubernetes concept which determines the scheduling and eviction priority of pods.
QoS class is used by the Kubernetes scheduler to make decisions about scheduling pods onto nodes.

QoS class is assigned to pods by Kubernetes itself. We can control the QoS class assigned to a pod by playing around with resource requests and limits for individual containers inside the pod.

There are three QoS classes in Kubernetes:

- Guaranteed

*** How Can I Assign a QoS class of Guaranteed to a Pod?

For a pod to be placed in the Guaranteed QoS class, every container in the pod must have a CPU and memory limit. Kubernetes will automatically assign CPU and memory request values (that are equal to the CPU and memory limit values) to the containers inside this pod and will assign it the Guaranteed QoS class.
Pods with explicit and equal values for both CPU requests and limits and memory requests and limits are also placed in the Guaranteed QoS class.

*** How does the Kubernetes Scheduler Handle Guaranteed Pods?

The Kubernetes scheduler assigns Guaranteed pods only to nodes which have enough resources to fulfil their CPU and memory requests. The Scheduler does this by ensuring that the sum of both memory and CPU requests for all containers (running and newly scheduled) is lower than the total capacity of the node.

- Burstable

*** How Can I assign a QoS class of Burstable to a Pod?

A pod is assigned a Burstable QoS class if at least one container in that pod has a memory or CPU request.

*** How does the Kubernetes Scheduler Handle Burstable Pods?

The Kubernetes scheduler will not be able to ensure that Burstable pods are placed onto nodes that have enough resources for them.

- BestEffort

*** How can I Assign a QoS Class of BestEffort to a Pod?

A pod is assigned a BestEffort QoS class if none of it’s containers have CPU or memory requests and limits.

*** How does the Kubernetes Scheduler handle BestEffort pods?

BestEffort pods are not guaranteed to be placed on to pods that have enough resources for them. They are, however, able to use any amount of free CPU and memory resources on the node.
This can at times lead to resource contention with other pods, where BestEffort pods hog resources and do not leave enough resource headroom for other pods to consume resources within resource limits.

* Static Pods

Static pods are a type of pod that is not observed or managed via kube-apiserver and is directly bound to the Kubelet daemon on the node.

* Assigning Pods to Nodes

Kubernetes scheduler ensures that the right node is selected by checking the node’s capacity for CPU and RAM and comparing it to the Pod’s resource requests. The scheduler makes sure that, for each of these resource types, the sum of all resource requests by the Pods’ containers is less than the capacity of the node. This mechanism ensures that Pods end up on nodes with spare resources.

However, there are some scenarios when you want your Pods to end up on specific nodes. For example:

    You want your Pod(s) to end up on a machine with the SSD attached to it.
    You want to co-locate Pods on a particular machine(s) from the same availability zone.
    You want to co-locate a Pod from one Service with a Pod from another service on the same node because these Services strongly depend on each other. For example, you may want to place a web server on the same node as the in-memory cache store like Memcached (see the example below).

These scenarios are addressed by a number of primitives in Kubernetes:

- nodeSelector

This is a simple Pod scheduling feature that allows scheduling a Pod onto a node whose labels match the nodeSelector labels specified by the user. nodeSelector is the early Kubernetes feature designed for manual Pod scheduling. The basic idea behind the nodeSelector is to allow a Pod to be scheduled only on those nodes that have label(s) identical to the label(s) defined in the nodeSelector. The latter are key-value pairs that can be specified inside the PodSpec.

- Node and Pod Affinity and Anti-Affinity

Node Affinity

This is the enhanced version of the nodeSelector introduced in Kubernetes 1.4. It offers a more expressive syntax for fine-grained control of how Pods are scheduled to specific nodes.
Node affinity allows scheduling Pods to specific nodes. There are a number of use cases for node affinity, including the following:
Spreading Pods across different availability zones to improve resilience and availability of applications in the cluster (see the image below).
Allocating nodes for memory-intensive Pods. In this case, you can have a few nodes dedicated to less compute-intensive Pods and one or two nodes with enough CPU and RAM dedicated to memory-intensive Pods.

One of the best features of the current affinity implementation in Kubernetes is the support for “hard” and “soft” node affinity.

With “hard” affinity, users can set a precise rule that should be met in order for a Pod to be scheduled on a node. For example, using “hard” affinity you can tell the scheduler to run the Pod only on the node that has SSD storage attached to it.

As the name suggests, “soft” affinity is less strict. Using “soft” affinity, you can ask the scheduler to try to run the set of Pod in availability zone XYZ, but if it’s impossible, allow some of these Pods to run in the other Availability Zone.

requiredDuringSchedulingIgnoredDuringExecution == hard
preferredDuringSchedulingIgnoredDuringExecution == soft

Inter-Pod Affinity/Anti-Affinity

This feature addresses the third scenario above. Inter-Pod affinity allows co-location by scheduling Pods onto nodes that already have specific Pods running.


* Taints and Toleration

Taints and Toleration are to restrict what pods to be schedule on what nodes

The nodes get taints and the pods are tolerate.

When the pods are created k8s scheduler schedule the pods oin any available nodes if it fullfil condition.
If we want specific type of pods to be schedule on a specific node we need to apply taint to a Node by default pods are intolernt which means once
we apply taint to a node no pod will be able to schedule on it as it is intolerant of taint.

To enable the certain pods on the taint node we will add the toleration to the pod which we want to schedule.

To apply the taint to a node we can use kubectl command

kubectl taint nodes node-name key=value:taint-effect

There are 3 types of taints

1. Noschedule - No scgeduling
2. PreferNo schedule - system will try to not schedule but no guarantee
3. No Excute - no new pod schedule


To add toleration to a pos we can add it in definition file just need to add the toleration in spec.

* Node Selector




* Node Affinity

The primary purpose is to ensure the pods are hosted on particular nodes.
